{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers,optimizers,datasets,Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Conv2D(4,kernel_size = 5 , strides =1 , padding = 'valid')\n",
    "#4:卷积核个数  padding也以为same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = tf.keras.layers.MaxPool2D(2,strides=2)\n",
    "#2:kernel_size:2 , "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.UpSampling2D(size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = layers.BatchNormalization()\n",
    "x = tf.random.normal([2,3])\n",
    "out = net(x)\n",
    "'''\n",
    "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
    "array([[ 0.3583169, -1.3122452, -1.2463319],\n",
    "        [-1.2777916, -1.7581975, -0.5057919]], dtype=float32)>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'batch_normalization/gamma:0' shape=(3,) dtype=float32, numpy=array([1., 1., 1.], dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization/beta:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization/moving_mean:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization/moving_variance:0' shape=(3,) dtype=float32, numpy=array([1., 1., 1.], dtype=float32)>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.trainable_variables\n",
    "'''\n",
    "[<tf.Variable 'batch_normalization/gamma:0' shape=(3,) dtype=float32, numpy=array([1., 1., 1.], dtype=float32)>,\n",
    "<tf.Variable 'batch_normalization/beta:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n",
    "'''\n",
    "\n",
    "net.variables\n",
    "'''\n",
    "[<tf.Variable 'batch_normalization/gamma:0' shape=(3,) dtype=float32, numpy=array([1., 1., 1.], dtype=float32)>,\n",
    " <tf.Variable 'batch_normalization/beta:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>,\n",
    " <tf.Variable 'batch_normalization/moving_mean:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>,\n",
    " <tf.Variable 'batch_normalization/moving_variance:0' shape=(3,) dtype=float32, numpy=array([1., 1., 1.], dtype=float32)>]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'batch_normalization_2/gamma:0' shape=(3,) dtype=float32, numpy=array([1., 1., 1.], dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_2/beta:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_2/moving_mean:0' shape=(3,) dtype=float32, numpy=array([-0.00855916, -0.01100943, -0.00956823], dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_2/moving_variance:0' shape=(3,) dtype=float32, numpy=array([0.99258375, 0.991314  , 0.9923488 ], dtype=float32)>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal([2,4,4,3],mean=-1,stddev=0.5)\n",
    "net = layers.BatchNormalization(axis=3)\n",
    "out = net(x,training=True)\n",
    "net.variables\n",
    "'''\n",
    "[<tf.Variable 'batch_normalization_2/gamma:0' shape=(3,) dtype=float32, numpy=array([1., 1., 1.], dtype=float32)>,\n",
    " <tf.Variable 'batch_normalization_2/beta:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>,\n",
    " <tf.Variable 'batch_normalization_2/moving_mean:0' shape=(3,) dtype=float32, numpy=array([-0.00855916, -0.01100943, -0.00956823], dtype=float32)>,\n",
    " <tf.Variable 'batch_normalization_2/moving_variance:0' shape=(3,) dtype=float32, numpy=array([0.99258375, 0.991314  , 0.9923488 ], dtype=float32)>]\n",
    "'''\n",
    "for i in range(100): out = net(x,training=True)\n",
    "\n",
    "net.variables\n",
    "'''\n",
    "[<tf.Variable 'batch_normalization_1/gamma:0' shape=(3,) dtype=float32, numpy=array([1., 1., 1.], dtype=float32)>,\n",
    " <tf.Variable 'batch_normalization_1/beta:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>,\n",
    " <tf.Variable 'batch_normalization_1/moving_mean:0' shape=(3,) dtype=float32, numpy=array([-0.6259307, -0.58605  , -0.6089093], dtype=float32)>,\n",
    " <tf.Variable 'batch_normalization_1/moving_variance:0' shape=(3,) dtype=float32, numpy=array([0.48389903, 0.50812393, 0.61608636], dtype=float32)>]\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         out = net(x,training=True)\n",
    "#         loss = tf.reduce_mean(tf.pow(out,2)) -1 \n",
    "#     grads = tape.gradient(loss,net.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(grads,net.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(layers.Layer):\n",
    "    def __init__(self,filter_num,stride = 1):\n",
    "        self.conv1 = layers.Conv2D(filter_num,(3,3),strides=stride,padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu = layers.Activation('relu')\n",
    "        self.conv2 = layers.Conv2D(filter_num,(3,3),strides = 1,padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        \n",
    "        #此步就是短接层，目的就是让两者的特征图可以相加！！！！\n",
    "        if stride != 1:\n",
    "            self.downsample = Sequential()\n",
    "            self.downsample.add(layers.Conv2D(filter_num,(1,1),strides=stride))\n",
    "            self.downsample.add(layers.BatchNormalization())\n",
    "        else:\n",
    "            self.downsample = lambda x:x\n",
    "        self.stride = stride\n",
    "    def call(self,inputs,training=None):\n",
    "        residual = self.downsample(inputs)\n",
    "        conv1 = self.conv1(inputs)\n",
    "        bn1 = self.bn1(conv1)\n",
    "        relu = self.relu(bn1)\n",
    "        conv2 = self.conv2(relu)\n",
    "        bn2 = self.bn2(conv2)\n",
    "        \n",
    "        add = layers.add([bn2,residual])\n",
    "        out = self.relu(add)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e8db0730c91600a5349938b5440aa07dc44fbf5b422a40c9df5fa4ade778ff1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
